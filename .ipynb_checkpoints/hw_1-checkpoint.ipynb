{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import keras\n",
    "import matplotlib.pyplot as plt #This package is for plotting\n",
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and splitting into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "bos_df = pd.DataFrame(boston.data)\n",
    "bos_df.columns = boston.feature_names\n",
    "bos_df['PRICE'] = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT       PRICE  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 2)\n",
      "(152, 2)\n"
     ]
    }
   ],
   "source": [
    "# X, y = bos_df.data, bos_df.target\n",
    "\n",
    "X1 = bos_df['LSTAT']\n",
    "X2 = bos_df['RM']\n",
    "\n",
    "X = np.array(list(zip(X1, X2)))\n",
    "y = bos_df['PRICE']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data before input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39111111],\n",
       "       [1.        ],\n",
       "       [0.4       ],\n",
       "       [0.07333333],\n",
       "       [0.36      ],\n",
       "       [0.33111111],\n",
       "       [0.34666667],\n",
       "       [0.30444444],\n",
       "       [0.24666667],\n",
       "       [0.30222222],\n",
       "       [0.08444444],\n",
       "       [0.27111111],\n",
       "       [0.22      ],\n",
       "       [0.12222222],\n",
       "       [1.        ],\n",
       "       [0.53333333],\n",
       "       [0.4       ],\n",
       "       [0.62888889],\n",
       "       [0.54222222],\n",
       "       [0.35555556],\n",
       "       [0.41777778],\n",
       "       [0.31333333],\n",
       "       [0.34222222],\n",
       "       [0.53555556],\n",
       "       [0.31777778],\n",
       "       [0.40222222],\n",
       "       [0.32444444],\n",
       "       [0.32      ],\n",
       "       [0.74888889],\n",
       "       [0.30444444],\n",
       "       [0.21333333],\n",
       "       [0.33333333],\n",
       "       [0.34444444],\n",
       "       [0.33555556],\n",
       "       [0.41333333],\n",
       "       [0.26222222],\n",
       "       [0.01333333],\n",
       "       [1.        ],\n",
       "       [0.21111111],\n",
       "       [0.18444444],\n",
       "       [0.42      ],\n",
       "       [0.33333333],\n",
       "       [0.32888889],\n",
       "       [0.19555556],\n",
       "       [0.25555556],\n",
       "       [0.36888889],\n",
       "       [0.34      ],\n",
       "       [0.26666667],\n",
       "       [0.15111111],\n",
       "       [0.5       ],\n",
       "       [0.23555556],\n",
       "       [0.40222222],\n",
       "       [0.42888889],\n",
       "       [0.84      ],\n",
       "       [0.23555556],\n",
       "       [0.37111111],\n",
       "       [0.26888889],\n",
       "       [0.27111111],\n",
       "       [0.22222222],\n",
       "       [0.37111111],\n",
       "       [0.30222222],\n",
       "       [0.35555556],\n",
       "       [0.62444444],\n",
       "       [0.58888889],\n",
       "       [0.33555556],\n",
       "       [0.55111111],\n",
       "       [0.22666667],\n",
       "       [0.22222222],\n",
       "       [0.5       ],\n",
       "       [0.39111111],\n",
       "       [0.33333333],\n",
       "       [0.36444444],\n",
       "       [0.41111111],\n",
       "       [0.58222222],\n",
       "       [0.41555556],\n",
       "       [0.05333333],\n",
       "       [0.96222222],\n",
       "       [0.43111111],\n",
       "       [0.39111111],\n",
       "       [0.29555556],\n",
       "       [0.40666667],\n",
       "       [0.26888889],\n",
       "       [0.50888889],\n",
       "       [0.88444444],\n",
       "       [1.        ],\n",
       "       [0.4       ],\n",
       "       [0.36444444],\n",
       "       [0.11555556],\n",
       "       [0.40666667],\n",
       "       [0.40444444],\n",
       "       [0.30888889],\n",
       "       [0.18666667],\n",
       "       [0.37555556],\n",
       "       [0.44      ],\n",
       "       [0.15333333],\n",
       "       [0.42888889],\n",
       "       [0.19555556],\n",
       "       [0.43777778],\n",
       "       [0.20222222],\n",
       "       [0.30444444],\n",
       "       [0.51333333],\n",
       "       [0.32888889],\n",
       "       [0.48222222],\n",
       "       [0.37111111],\n",
       "       [0.37777778],\n",
       "       [0.39777778],\n",
       "       [0.12      ],\n",
       "       [0.37555556],\n",
       "       [0.34666667],\n",
       "       [0.47555556],\n",
       "       [0.80666667],\n",
       "       [0.27111111],\n",
       "       [0.49111111],\n",
       "       [0.34222222],\n",
       "       [0.25555556],\n",
       "       [0.43111111],\n",
       "       [0.07555556],\n",
       "       [0.4       ],\n",
       "       [0.10444444],\n",
       "       [1.        ],\n",
       "       [0.56666667],\n",
       "       [0.16222222],\n",
       "       [0.32      ],\n",
       "       [0.36      ],\n",
       "       [0.34      ],\n",
       "       [0.30666667],\n",
       "       [0.63111111],\n",
       "       [0.3       ],\n",
       "       [0.32444444],\n",
       "       [0.62666667],\n",
       "       [0.18      ],\n",
       "       [0.05555556],\n",
       "       [0.19111111],\n",
       "       [0.27555556],\n",
       "       [0.07555556],\n",
       "       [0.67555556],\n",
       "       [0.42222222],\n",
       "       [0.18666667],\n",
       "       [0.47111111],\n",
       "       [0.04888889],\n",
       "       [0.18      ],\n",
       "       [0.43333333],\n",
       "       [0.71555556],\n",
       "       [0.44444444],\n",
       "       [0.42444444],\n",
       "       [0.25777778],\n",
       "       [0.62      ],\n",
       "       [0.69333333],\n",
       "       [0.13333333],\n",
       "       [0.04888889],\n",
       "       [0.39555556],\n",
       "       [0.52666667]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_x = MinMaxScaler()\n",
    "sc_x.fit(X_train)\n",
    "sc_x.transform(X_test)\n",
    "\n",
    "sc_y = MinMaxScaler()\n",
    "y_train = y_train.values.reshape(-1,1)\n",
    "y_test = y_test.values.reshape(-1,1)\n",
    "sc_y.fit(y_train)\n",
    "sc_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim = 2, kernel_initializer='normal'))\n",
    "    model.add(Activation('linear'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', metrics=['mse'], optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "def fit_model():\n",
    "    return model.fit(X_train, y_train, epochs=150, batch_size=50,  verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict():\n",
    "    y_pred = model.predict(X_test)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(y_test, y_pred)\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_nn():\n",
    "    model = build_model()\n",
    "    fit_model()\n",
    "    model_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 283 samples, validate on 71 samples\n",
      "Epoch 1/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 655.4874 - mean_squared_error: 655.4874 - val_loss: 604.2588 - val_mean_squared_error: 604.2588\n",
      "Epoch 2/150\n",
      "283/283 [==============================] - 0s 38us/step - loss: 629.7797 - mean_squared_error: 629.7797 - val_loss: 580.0064 - val_mean_squared_error: 580.0064\n",
      "Epoch 3/150\n",
      "283/283 [==============================] - 0s 75us/step - loss: 604.8578 - mean_squared_error: 604.8578 - val_loss: 556.5053 - val_mean_squared_error: 556.5053\n",
      "Epoch 4/150\n",
      "283/283 [==============================] - 0s 64us/step - loss: 580.8289 - mean_squared_error: 580.8289 - val_loss: 533.8507 - val_mean_squared_error: 533.8507\n",
      "Epoch 5/150\n",
      "283/283 [==============================] - 0s 62us/step - loss: 557.6103 - mean_squared_error: 557.6103 - val_loss: 512.0406 - val_mean_squared_error: 512.0406\n",
      "Epoch 6/150\n",
      "283/283 [==============================] - 0s 57us/step - loss: 535.0411 - mean_squared_error: 535.0411 - val_loss: 491.0482 - val_mean_squared_error: 491.0482\n",
      "Epoch 7/150\n",
      "283/283 [==============================] - 0s 41us/step - loss: 513.3050 - mean_squared_error: 513.3050 - val_loss: 470.7784 - val_mean_squared_error: 470.7784\n",
      "Epoch 8/150\n",
      "283/283 [==============================] - 0s 55us/step - loss: 492.2744 - mean_squared_error: 492.2744 - val_loss: 451.1178 - val_mean_squared_error: 451.1178\n",
      "Epoch 9/150\n",
      "283/283 [==============================] - 0s 45us/step - loss: 471.8309 - mean_squared_error: 471.8309 - val_loss: 432.1115 - val_mean_squared_error: 432.1115\n",
      "Epoch 10/150\n",
      "283/283 [==============================] - 0s 43us/step - loss: 452.1618 - mean_squared_error: 452.1618 - val_loss: 413.6703 - val_mean_squared_error: 413.6703\n",
      "Epoch 11/150\n",
      "283/283 [==============================] - 0s 43us/step - loss: 432.9616 - mean_squared_error: 432.9616 - val_loss: 395.9028 - val_mean_squared_error: 395.9028\n",
      "Epoch 12/150\n",
      "283/283 [==============================] - 0s 64us/step - loss: 414.3767 - mean_squared_error: 414.3767 - val_loss: 378.7599 - val_mean_squared_error: 378.7599\n",
      "Epoch 13/150\n",
      "283/283 [==============================] - 0s 57us/step - loss: 396.2872 - mean_squared_error: 396.2872 - val_loss: 362.2944 - val_mean_squared_error: 362.2944\n",
      "Epoch 14/150\n",
      "283/283 [==============================] - 0s 65us/step - loss: 379.0269 - mean_squared_error: 379.0269 - val_loss: 346.4059 - val_mean_squared_error: 346.4059\n",
      "Epoch 15/150\n",
      "283/283 [==============================] - 0s 60us/step - loss: 362.1509 - mean_squared_error: 362.1509 - val_loss: 331.1907 - val_mean_squared_error: 331.1907\n",
      "Epoch 16/150\n",
      "283/283 [==============================] - 0s 53us/step - loss: 345.7348 - mean_squared_error: 345.7348 - val_loss: 316.7517 - val_mean_squared_error: 316.7517\n",
      "Epoch 17/150\n",
      "283/283 [==============================] - 0s 62us/step - loss: 330.5979 - mean_squared_error: 330.5979 - val_loss: 302.9553 - val_mean_squared_error: 302.9553\n",
      "Epoch 18/150\n",
      "283/283 [==============================] - 0s 47us/step - loss: 315.7846 - mean_squared_error: 315.7846 - val_loss: 290.0629 - val_mean_squared_error: 290.0629\n",
      "Epoch 19/150\n",
      "283/283 [==============================] - 0s 104us/step - loss: 302.0808 - mean_squared_error: 302.0808 - val_loss: 277.9599 - val_mean_squared_error: 277.9599\n",
      "Epoch 20/150\n",
      "283/283 [==============================] - 0s 66us/step - loss: 288.9008 - mean_squared_error: 288.9008 - val_loss: 266.8664 - val_mean_squared_error: 266.8664\n",
      "Epoch 21/150\n",
      "283/283 [==============================] - 0s 52us/step - loss: 277.0529 - mean_squared_error: 277.0529 - val_loss: 256.5916 - val_mean_squared_error: 256.5916\n",
      "Epoch 22/150\n",
      "283/283 [==============================] - 0s 52us/step - loss: 265.8639 - mean_squared_error: 265.8639 - val_loss: 247.3366 - val_mean_squared_error: 247.3366\n",
      "Epoch 23/150\n",
      "283/283 [==============================] - 0s 70us/step - loss: 255.6088 - mean_squared_error: 255.6088 - val_loss: 239.0849 - val_mean_squared_error: 239.0849\n",
      "Epoch 24/150\n",
      "283/283 [==============================] - 0s 48us/step - loss: 246.8184 - mean_squared_error: 246.8184 - val_loss: 231.5972 - val_mean_squared_error: 231.5972\n",
      "Epoch 25/150\n",
      "283/283 [==============================] - 0s 44us/step - loss: 238.3419 - mean_squared_error: 238.3419 - val_loss: 225.0403 - val_mean_squared_error: 225.0403\n",
      "Epoch 26/150\n",
      "283/283 [==============================] - 0s 55us/step - loss: 230.9772 - mean_squared_error: 230.9772 - val_loss: 219.2763 - val_mean_squared_error: 219.2763\n",
      "Epoch 27/150\n",
      "283/283 [==============================] - 0s 45us/step - loss: 224.5212 - mean_squared_error: 224.5212 - val_loss: 214.2432 - val_mean_squared_error: 214.2432\n",
      "Epoch 28/150\n",
      "283/283 [==============================] - 0s 59us/step - loss: 218.5969 - mean_squared_error: 218.5969 - val_loss: 209.9079 - val_mean_squared_error: 209.9079\n",
      "Epoch 29/150\n",
      "283/283 [==============================] - 0s 45us/step - loss: 213.6385 - mean_squared_error: 213.6385 - val_loss: 205.9903 - val_mean_squared_error: 205.9903\n",
      "Epoch 30/150\n",
      "283/283 [==============================] - 0s 48us/step - loss: 208.9825 - mean_squared_error: 208.9825 - val_loss: 202.5720 - val_mean_squared_error: 202.5720\n",
      "Epoch 31/150\n",
      "283/283 [==============================] - 0s 45us/step - loss: 204.9374 - mean_squared_error: 204.9374 - val_loss: 199.4685 - val_mean_squared_error: 199.4685\n",
      "Epoch 32/150\n",
      "283/283 [==============================] - 0s 49us/step - loss: 201.2412 - mean_squared_error: 201.2412 - val_loss: 196.6482 - val_mean_squared_error: 196.6482\n",
      "Epoch 33/150\n",
      "283/283 [==============================] - 0s 38us/step - loss: 198.0392 - mean_squared_error: 198.0392 - val_loss: 194.0847 - val_mean_squared_error: 194.0847\n",
      "Epoch 34/150\n",
      "283/283 [==============================] - 0s 38us/step - loss: 194.8088 - mean_squared_error: 194.8088 - val_loss: 191.6760 - val_mean_squared_error: 191.6760\n",
      "Epoch 35/150\n",
      "283/283 [==============================] - 0s 38us/step - loss: 192.0316 - mean_squared_error: 192.0316 - val_loss: 189.3569 - val_mean_squared_error: 189.3569\n",
      "Epoch 36/150\n",
      "283/283 [==============================] - 0s 49us/step - loss: 189.2448 - mean_squared_error: 189.2448 - val_loss: 187.1112 - val_mean_squared_error: 187.1112\n",
      "Epoch 37/150\n",
      "283/283 [==============================] - 0s 72us/step - loss: 186.7765 - mean_squared_error: 186.7765 - val_loss: 184.8304 - val_mean_squared_error: 184.8304\n",
      "Epoch 38/150\n",
      "283/283 [==============================] - 0s 58us/step - loss: 184.2827 - mean_squared_error: 184.2827 - val_loss: 182.5307 - val_mean_squared_error: 182.5307\n",
      "Epoch 39/150\n",
      "283/283 [==============================] - 0s 53us/step - loss: 181.6746 - mean_squared_error: 181.6746 - val_loss: 180.2144 - val_mean_squared_error: 180.2144\n",
      "Epoch 40/150\n",
      "283/283 [==============================] - 0s 50us/step - loss: 179.2047 - mean_squared_error: 179.2047 - val_loss: 177.9375 - val_mean_squared_error: 177.9375\n",
      "Epoch 41/150\n",
      "283/283 [==============================] - 0s 54us/step - loss: 176.7754 - mean_squared_error: 176.7754 - val_loss: 175.6131 - val_mean_squared_error: 175.6131\n",
      "Epoch 42/150\n",
      "283/283 [==============================] - 0s 53us/step - loss: 174.3177 - mean_squared_error: 174.3177 - val_loss: 173.1767 - val_mean_squared_error: 173.1767\n",
      "Epoch 43/150\n",
      "283/283 [==============================] - 0s 45us/step - loss: 171.7848 - mean_squared_error: 171.7848 - val_loss: 170.7507 - val_mean_squared_error: 170.7507\n",
      "Epoch 44/150\n",
      "283/283 [==============================] - 0s 39us/step - loss: 169.3125 - mean_squared_error: 169.3125 - val_loss: 168.2286 - val_mean_squared_error: 168.2286\n",
      "Epoch 45/150\n",
      "283/283 [==============================] - 0s 43us/step - loss: 166.6940 - mean_squared_error: 166.6940 - val_loss: 165.7640 - val_mean_squared_error: 165.7640\n",
      "Epoch 46/150\n",
      "283/283 [==============================] - 0s 38us/step - loss: 164.2859 - mean_squared_error: 164.2859 - val_loss: 163.1449 - val_mean_squared_error: 163.1449\n",
      "Epoch 47/150\n",
      "283/283 [==============================] - 0s 44us/step - loss: 161.6375 - mean_squared_error: 161.6375 - val_loss: 160.5872 - val_mean_squared_error: 160.5872\n",
      "Epoch 48/150\n",
      "283/283 [==============================] - 0s 48us/step - loss: 159.0916 - mean_squared_error: 159.0916 - val_loss: 157.9844 - val_mean_squared_error: 157.9844\n",
      "Epoch 49/150\n",
      "283/283 [==============================] - 0s 40us/step - loss: 156.4959 - mean_squared_error: 156.4959 - val_loss: 155.3358 - val_mean_squared_error: 155.3358\n",
      "Epoch 50/150\n",
      "283/283 [==============================] - 0s 42us/step - loss: 153.9129 - mean_squared_error: 153.9129 - val_loss: 152.5987 - val_mean_squared_error: 152.5987\n",
      "Epoch 51/150\n",
      "283/283 [==============================] - 0s 41us/step - loss: 151.2650 - mean_squared_error: 151.2650 - val_loss: 149.8147 - val_mean_squared_error: 149.8147\n",
      "Epoch 52/150\n",
      "283/283 [==============================] - 0s 39us/step - loss: 148.6863 - mean_squared_error: 148.6863 - val_loss: 147.0877 - val_mean_squared_error: 147.0877\n",
      "Epoch 53/150\n",
      "283/283 [==============================] - 0s 42us/step - loss: 145.8912 - mean_squared_error: 145.8912 - val_loss: 144.3529 - val_mean_squared_error: 144.3529\n",
      "Epoch 54/150\n",
      "283/283 [==============================] - 0s 36us/step - loss: 143.2765 - mean_squared_error: 143.2765 - val_loss: 141.5935 - val_mean_squared_error: 141.5935\n",
      "Epoch 55/150\n",
      "283/283 [==============================] - 0s 37us/step - loss: 140.5743 - mean_squared_error: 140.5743 - val_loss: 138.8530 - val_mean_squared_error: 138.8530\n",
      "Epoch 56/150\n",
      "283/283 [==============================] - 0s 41us/step - loss: 137.8551 - mean_squared_error: 137.8551 - val_loss: 136.0271 - val_mean_squared_error: 136.0271\n",
      "Epoch 57/150\n",
      "283/283 [==============================] - 0s 41us/step - loss: 135.1831 - mean_squared_error: 135.1831 - val_loss: 133.1480 - val_mean_squared_error: 133.1480\n",
      "Epoch 58/150\n",
      "283/283 [==============================] - 0s 39us/step - loss: 132.3918 - mean_squared_error: 132.3918 - val_loss: 130.3562 - val_mean_squared_error: 130.3562\n",
      "Epoch 59/150\n",
      "283/283 [==============================] - 0s 37us/step - loss: 129.7102 - mean_squared_error: 129.7102 - val_loss: 127.5178 - val_mean_squared_error: 127.5178\n",
      "Epoch 60/150\n",
      "283/283 [==============================] - 0s 39us/step - loss: 126.9815 - mean_squared_error: 126.9815 - val_loss: 124.7535 - val_mean_squared_error: 124.7535\n",
      "Epoch 61/150\n",
      "283/283 [==============================] - 0s 39us/step - loss: 124.1845 - mean_squared_error: 124.1845 - val_loss: 121.9256 - val_mean_squared_error: 121.9256\n",
      "Epoch 62/150\n",
      "283/283 [==============================] - 0s 40us/step - loss: 121.5152 - mean_squared_error: 121.5152 - val_loss: 119.0985 - val_mean_squared_error: 119.0985\n",
      "Epoch 63/150\n",
      "283/283 [==============================] - 0s 56us/step - loss: 118.8099 - mean_squared_error: 118.8099 - val_loss: 116.3111 - val_mean_squared_error: 116.3111\n",
      "Epoch 64/150\n",
      "283/283 [==============================] - 0s 45us/step - loss: 116.0593 - mean_squared_error: 116.0593 - val_loss: 113.4237 - val_mean_squared_error: 113.4237\n",
      "Epoch 65/150\n",
      "283/283 [==============================] - 0s 50us/step - loss: 113.4006 - mean_squared_error: 113.4006 - val_loss: 110.5487 - val_mean_squared_error: 110.5487\n",
      "Epoch 66/150\n",
      "283/283 [==============================] - 0s 53us/step - loss: 110.7232 - mean_squared_error: 110.7232 - val_loss: 107.7334 - val_mean_squared_error: 107.7334\n",
      "Epoch 67/150\n",
      "283/283 [==============================] - 0s 49us/step - loss: 108.0246 - mean_squared_error: 108.0246 - val_loss: 104.9653 - val_mean_squared_error: 104.9653\n",
      "Epoch 68/150\n",
      "283/283 [==============================] - 0s 51us/step - loss: 105.2580 - mean_squared_error: 105.2580 - val_loss: 102.2669 - val_mean_squared_error: 102.2669\n",
      "Epoch 69/150\n",
      "283/283 [==============================] - 0s 52us/step - loss: 102.8109 - mean_squared_error: 102.8109 - val_loss: 99.4593 - val_mean_squared_error: 99.4593\n",
      "Epoch 70/150\n",
      "283/283 [==============================] - 0s 52us/step - loss: 100.1107 - mean_squared_error: 100.1107 - val_loss: 96.7784 - val_mean_squared_error: 96.7784\n",
      "Epoch 71/150\n",
      "283/283 [==============================] - 0s 41us/step - loss: 97.5202 - mean_squared_error: 97.5202 - val_loss: 94.1144 - val_mean_squared_error: 94.1144\n",
      "Epoch 72/150\n",
      "283/283 [==============================] - 0s 41us/step - loss: 94.9962 - mean_squared_error: 94.9962 - val_loss: 91.5298 - val_mean_squared_error: 91.5298\n",
      "Epoch 73/150\n",
      "283/283 [==============================] - 0s 43us/step - loss: 92.4958 - mean_squared_error: 92.4958 - val_loss: 88.9735 - val_mean_squared_error: 88.9735\n",
      "Epoch 74/150\n",
      "283/283 [==============================] - 0s 45us/step - loss: 90.0594 - mean_squared_error: 90.0594 - val_loss: 86.3034 - val_mean_squared_error: 86.3034\n",
      "Epoch 75/150\n",
      "283/283 [==============================] - 0s 39us/step - loss: 87.5834 - mean_squared_error: 87.5834 - val_loss: 83.7377 - val_mean_squared_error: 83.7377\n",
      "Epoch 76/150\n",
      "283/283 [==============================] - 0s 41us/step - loss: 85.2361 - mean_squared_error: 85.2361 - val_loss: 81.2222 - val_mean_squared_error: 81.2222\n",
      "Epoch 77/150\n",
      "283/283 [==============================] - 0s 40us/step - loss: 82.8736 - mean_squared_error: 82.8736 - val_loss: 78.8370 - val_mean_squared_error: 78.8370\n",
      "Epoch 78/150\n",
      "283/283 [==============================] - 0s 40us/step - loss: 80.5360 - mean_squared_error: 80.5360 - val_loss: 76.5151 - val_mean_squared_error: 76.5151\n",
      "Epoch 79/150\n",
      "283/283 [==============================] - 0s 48us/step - loss: 78.2867 - mean_squared_error: 78.2867 - val_loss: 74.1707 - val_mean_squared_error: 74.1707\n",
      "Epoch 80/150\n",
      "283/283 [==============================] - 0s 41us/step - loss: 76.2076 - mean_squared_error: 76.2076 - val_loss: 71.7964 - val_mean_squared_error: 71.7964\n",
      "Epoch 81/150\n",
      "283/283 [==============================] - 0s 43us/step - loss: 73.9522 - mean_squared_error: 73.9522 - val_loss: 69.5397 - val_mean_squared_error: 69.5397\n",
      "Epoch 82/150\n",
      "283/283 [==============================] - 0s 38us/step - loss: 71.8971 - mean_squared_error: 71.8971 - val_loss: 67.3729 - val_mean_squared_error: 67.3729\n",
      "Epoch 83/150\n",
      "283/283 [==============================] - 0s 39us/step - loss: 69.8505 - mean_squared_error: 69.8505 - val_loss: 65.2158 - val_mean_squared_error: 65.2158\n",
      "Epoch 84/150\n",
      "283/283 [==============================] - 0s 56us/step - loss: 67.8262 - mean_squared_error: 67.8262 - val_loss: 63.2198 - val_mean_squared_error: 63.2198\n",
      "Epoch 85/150\n",
      "283/283 [==============================] - 0s 69us/step - loss: 65.9146 - mean_squared_error: 65.9146 - val_loss: 61.2130 - val_mean_squared_error: 61.2130\n",
      "Epoch 86/150\n",
      "283/283 [==============================] - 0s 43us/step - loss: 64.0576 - mean_squared_error: 64.0576 - val_loss: 59.2081 - val_mean_squared_error: 59.2081\n",
      "Epoch 87/150\n",
      "283/283 [==============================] - 0s 39us/step - loss: 62.2653 - mean_squared_error: 62.2653 - val_loss: 57.2513 - val_mean_squared_error: 57.2513\n",
      "Epoch 88/150\n",
      "283/283 [==============================] - 0s 40us/step - loss: 60.4665 - mean_squared_error: 60.4665 - val_loss: 55.4127 - val_mean_squared_error: 55.4127\n",
      "Epoch 89/150\n",
      "283/283 [==============================] - 0s 39us/step - loss: 58.7726 - mean_squared_error: 58.7726 - val_loss: 53.5885 - val_mean_squared_error: 53.5885\n",
      "Epoch 90/150\n",
      "283/283 [==============================] - 0s 38us/step - loss: 57.1637 - mean_squared_error: 57.1637 - val_loss: 51.8176 - val_mean_squared_error: 51.8176\n",
      "Epoch 91/150\n",
      "283/283 [==============================] - 0s 39us/step - loss: 55.6308 - mean_squared_error: 55.6308 - val_loss: 50.1991 - val_mean_squared_error: 50.1991\n",
      "Epoch 92/150\n",
      "283/283 [==============================] - 0s 40us/step - loss: 54.1602 - mean_squared_error: 54.1602 - val_loss: 48.6034 - val_mean_squared_error: 48.6034\n",
      "Epoch 93/150\n",
      "283/283 [==============================] - 0s 39us/step - loss: 52.6920 - mean_squared_error: 52.6920 - val_loss: 47.0758 - val_mean_squared_error: 47.0758\n",
      "Epoch 94/150\n",
      "283/283 [==============================] - 0s 38us/step - loss: 51.3348 - mean_squared_error: 51.3348 - val_loss: 45.6424 - val_mean_squared_error: 45.6424\n",
      "Epoch 95/150\n",
      "283/283 [==============================] - 0s 41us/step - loss: 50.0383 - mean_squared_error: 50.0383 - val_loss: 44.2859 - val_mean_squared_error: 44.2859\n",
      "Epoch 96/150\n",
      "283/283 [==============================] - 0s 41us/step - loss: 48.7436 - mean_squared_error: 48.7436 - val_loss: 43.0133 - val_mean_squared_error: 43.0133\n",
      "Epoch 97/150\n",
      "283/283 [==============================] - 0s 42us/step - loss: 47.6459 - mean_squared_error: 47.6459 - val_loss: 41.7784 - val_mean_squared_error: 41.7784\n",
      "Epoch 98/150\n",
      "283/283 [==============================] - 0s 42us/step - loss: 46.4873 - mean_squared_error: 46.4873 - val_loss: 40.6228 - val_mean_squared_error: 40.6228\n",
      "Epoch 99/150\n",
      "283/283 [==============================] - 0s 40us/step - loss: 45.3981 - mean_squared_error: 45.3981 - val_loss: 39.4827 - val_mean_squared_error: 39.4827\n",
      "Epoch 100/150\n",
      "283/283 [==============================] - 0s 45us/step - loss: 44.3683 - mean_squared_error: 44.3683 - val_loss: 38.3849 - val_mean_squared_error: 38.3849\n",
      "Epoch 101/150\n",
      "283/283 [==============================] - 0s 38us/step - loss: 43.4364 - mean_squared_error: 43.4364 - val_loss: 37.3535 - val_mean_squared_error: 37.3535\n",
      "Epoch 102/150\n",
      "283/283 [==============================] - 0s 37us/step - loss: 42.5685 - mean_squared_error: 42.5685 - val_loss: 36.3741 - val_mean_squared_error: 36.3741\n",
      "Epoch 103/150\n",
      "283/283 [==============================] - 0s 40us/step - loss: 41.6334 - mean_squared_error: 41.6334 - val_loss: 35.4003 - val_mean_squared_error: 35.4003\n",
      "Epoch 104/150\n",
      "283/283 [==============================] - 0s 40us/step - loss: 40.8608 - mean_squared_error: 40.8608 - val_loss: 34.4619 - val_mean_squared_error: 34.4619\n",
      "Epoch 105/150\n",
      "283/283 [==============================] - 0s 38us/step - loss: 40.0791 - mean_squared_error: 40.0791 - val_loss: 33.5759 - val_mean_squared_error: 33.5759\n",
      "Epoch 106/150\n",
      "283/283 [==============================] - 0s 38us/step - loss: 39.4517 - mean_squared_error: 39.4517 - val_loss: 32.8062 - val_mean_squared_error: 32.8062\n",
      "Epoch 107/150\n",
      "283/283 [==============================] - 0s 41us/step - loss: 38.7031 - mean_squared_error: 38.7031 - val_loss: 32.0863 - val_mean_squared_error: 32.0863\n",
      "Epoch 108/150\n",
      "283/283 [==============================] - 0s 40us/step - loss: 38.1447 - mean_squared_error: 38.1447 - val_loss: 31.4177 - val_mean_squared_error: 31.4177\n",
      "Epoch 109/150\n",
      "283/283 [==============================] - 0s 41us/step - loss: 37.6113 - mean_squared_error: 37.6113 - val_loss: 30.7780 - val_mean_squared_error: 30.7780\n",
      "Epoch 110/150\n",
      "283/283 [==============================] - 0s 40us/step - loss: 36.9909 - mean_squared_error: 36.9909 - val_loss: 30.2728 - val_mean_squared_error: 30.2728\n",
      "Epoch 111/150\n",
      "283/283 [==============================] - 0s 39us/step - loss: 36.5372 - mean_squared_error: 36.5372 - val_loss: 29.7369 - val_mean_squared_error: 29.7369\n",
      "Epoch 112/150\n",
      "283/283 [==============================] - 0s 40us/step - loss: 36.1718 - mean_squared_error: 36.1718 - val_loss: 29.2507 - val_mean_squared_error: 29.2507\n",
      "Epoch 113/150\n",
      "283/283 [==============================] - 0s 42us/step - loss: 35.6969 - mean_squared_error: 35.6969 - val_loss: 28.6565 - val_mean_squared_error: 28.6565\n",
      "Epoch 114/150\n",
      "283/283 [==============================] - 0s 38us/step - loss: 35.3083 - mean_squared_error: 35.3083 - val_loss: 28.2104 - val_mean_squared_error: 28.2104\n",
      "Epoch 115/150\n",
      "283/283 [==============================] - 0s 41us/step - loss: 34.9343 - mean_squared_error: 34.9343 - val_loss: 27.8290 - val_mean_squared_error: 27.8290\n",
      "Epoch 116/150\n",
      "283/283 [==============================] - 0s 45us/step - loss: 34.6298 - mean_squared_error: 34.6298 - val_loss: 27.4560 - val_mean_squared_error: 27.4560\n",
      "Epoch 117/150\n",
      "283/283 [==============================] - 0s 47us/step - loss: 34.3792 - mean_squared_error: 34.3792 - val_loss: 27.0332 - val_mean_squared_error: 27.0332\n",
      "Epoch 118/150\n",
      "283/283 [==============================] - 0s 59us/step - loss: 34.0466 - mean_squared_error: 34.0466 - val_loss: 26.7561 - val_mean_squared_error: 26.7561\n",
      "Epoch 119/150\n",
      "283/283 [==============================] - 0s 63us/step - loss: 33.8118 - mean_squared_error: 33.8118 - val_loss: 26.4669 - val_mean_squared_error: 26.4669\n",
      "Epoch 120/150\n",
      "283/283 [==============================] - 0s 62us/step - loss: 33.5828 - mean_squared_error: 33.5828 - val_loss: 26.2061 - val_mean_squared_error: 26.2061\n",
      "Epoch 121/150\n",
      "283/283 [==============================] - 0s 61us/step - loss: 33.3806 - mean_squared_error: 33.3806 - val_loss: 25.9443 - val_mean_squared_error: 25.9443\n",
      "Epoch 122/150\n",
      "283/283 [==============================] - 0s 67us/step - loss: 33.2114 - mean_squared_error: 33.2114 - val_loss: 25.6640 - val_mean_squared_error: 25.6640\n",
      "Epoch 123/150\n",
      "283/283 [==============================] - 0s 60us/step - loss: 33.0098 - mean_squared_error: 33.0098 - val_loss: 25.4828 - val_mean_squared_error: 25.4828\n",
      "Epoch 124/150\n",
      "283/283 [==============================] - 0s 52us/step - loss: 32.8738 - mean_squared_error: 32.8738 - val_loss: 25.2637 - val_mean_squared_error: 25.2637\n",
      "Epoch 125/150\n",
      "283/283 [==============================] - 0s 51us/step - loss: 32.7236 - mean_squared_error: 32.7236 - val_loss: 25.0958 - val_mean_squared_error: 25.0958\n",
      "Epoch 126/150\n",
      "283/283 [==============================] - 0s 63us/step - loss: 32.5979 - mean_squared_error: 32.5979 - val_loss: 24.9298 - val_mean_squared_error: 24.9298\n",
      "Epoch 127/150\n",
      "283/283 [==============================] - 0s 49us/step - loss: 32.4720 - mean_squared_error: 32.4720 - val_loss: 24.8152 - val_mean_squared_error: 24.8152\n",
      "Epoch 128/150\n",
      "283/283 [==============================] - 0s 55us/step - loss: 32.3935 - mean_squared_error: 32.3935 - val_loss: 24.7283 - val_mean_squared_error: 24.7283\n",
      "Epoch 129/150\n",
      "283/283 [==============================] - 0s 50us/step - loss: 32.2818 - mean_squared_error: 32.2818 - val_loss: 24.5537 - val_mean_squared_error: 24.5537\n",
      "Epoch 130/150\n",
      "283/283 [==============================] - 0s 54us/step - loss: 32.2062 - mean_squared_error: 32.2062 - val_loss: 24.4155 - val_mean_squared_error: 24.4155\n",
      "Epoch 131/150\n",
      "283/283 [==============================] - 0s 53us/step - loss: 32.1233 - mean_squared_error: 32.1233 - val_loss: 24.2686 - val_mean_squared_error: 24.2686\n",
      "Epoch 132/150\n",
      "283/283 [==============================] - 0s 47us/step - loss: 32.0644 - mean_squared_error: 32.0644 - val_loss: 24.1575 - val_mean_squared_error: 24.1575\n",
      "Epoch 133/150\n",
      "283/283 [==============================] - 0s 48us/step - loss: 32.0130 - mean_squared_error: 32.0130 - val_loss: 24.0390 - val_mean_squared_error: 24.0390\n",
      "Epoch 134/150\n",
      "283/283 [==============================] - 0s 53us/step - loss: 31.9634 - mean_squared_error: 31.9634 - val_loss: 23.9283 - val_mean_squared_error: 23.9283\n",
      "Epoch 135/150\n",
      "283/283 [==============================] - 0s 52us/step - loss: 31.9223 - mean_squared_error: 31.9223 - val_loss: 23.8843 - val_mean_squared_error: 23.8843\n",
      "Epoch 136/150\n",
      "283/283 [==============================] - 0s 57us/step - loss: 31.9008 - mean_squared_error: 31.9008 - val_loss: 23.8131 - val_mean_squared_error: 23.8131\n",
      "Epoch 137/150\n",
      "283/283 [==============================] - 0s 74us/step - loss: 31.8373 - mean_squared_error: 31.8373 - val_loss: 23.8018 - val_mean_squared_error: 23.8018\n",
      "Epoch 138/150\n",
      "283/283 [==============================] - 0s 47us/step - loss: 31.8183 - mean_squared_error: 31.8183 - val_loss: 23.8216 - val_mean_squared_error: 23.8216\n",
      "Epoch 139/150\n",
      "283/283 [==============================] - 0s 47us/step - loss: 31.7915 - mean_squared_error: 31.7915 - val_loss: 23.7869 - val_mean_squared_error: 23.7869\n",
      "Epoch 140/150\n",
      "283/283 [==============================] - 0s 49us/step - loss: 31.7689 - mean_squared_error: 31.7689 - val_loss: 23.7156 - val_mean_squared_error: 23.7156\n",
      "Epoch 141/150\n",
      "283/283 [==============================] - 0s 53us/step - loss: 31.7537 - mean_squared_error: 31.7537 - val_loss: 23.6567 - val_mean_squared_error: 23.6567\n",
      "Epoch 142/150\n",
      "283/283 [==============================] - 0s 47us/step - loss: 31.7145 - mean_squared_error: 31.7145 - val_loss: 23.5556 - val_mean_squared_error: 23.5556\n",
      "Epoch 143/150\n",
      "283/283 [==============================] - 0s 57us/step - loss: 31.7018 - mean_squared_error: 31.7018 - val_loss: 23.4755 - val_mean_squared_error: 23.4755\n",
      "Epoch 144/150\n",
      "283/283 [==============================] - 0s 45us/step - loss: 31.6909 - mean_squared_error: 31.6909 - val_loss: 23.4175 - val_mean_squared_error: 23.4175\n",
      "Epoch 145/150\n",
      "283/283 [==============================] - 0s 56us/step - loss: 31.6801 - mean_squared_error: 31.6801 - val_loss: 23.3786 - val_mean_squared_error: 23.3786\n",
      "Epoch 146/150\n",
      "283/283 [==============================] - 0s 57us/step - loss: 31.6688 - mean_squared_error: 31.6688 - val_loss: 23.3444 - val_mean_squared_error: 23.3444\n",
      "Epoch 147/150\n",
      "283/283 [==============================] - 0s 59us/step - loss: 31.6662 - mean_squared_error: 31.6662 - val_loss: 23.4426 - val_mean_squared_error: 23.4426\n",
      "Epoch 148/150\n",
      "283/283 [==============================] - 0s 54us/step - loss: 31.6424 - mean_squared_error: 31.6424 - val_loss: 23.4024 - val_mean_squared_error: 23.4024\n",
      "Epoch 149/150\n",
      "283/283 [==============================] - 0s 52us/step - loss: 31.6276 - mean_squared_error: 31.6276 - val_loss: 23.3923 - val_mean_squared_error: 23.3923\n",
      "Epoch 150/150\n",
      "283/283 [==============================] - 0s 59us/step - loss: 31.6278 - mean_squared_error: 31.6278 - val_loss: 23.4006 - val_mean_squared_error: 23.4006\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "keras_nn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and splitting into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load in dataset\n",
    "df_2 = pd.read_csv('./datasets/diabetes.csv')\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_2.loc[:, df_2.columns != 'Outcome']\n",
    "y = df_2.loc[:, 'Outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(537, 8)\n",
      "(231, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data before input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05882353, 1.00505051, 0.62295082, ..., 0.63934426, 0.56078767,\n",
       "        0.01666667],\n",
       "       [0.11764706, 0.54040404, 0.60655738, ..., 0.50074516, 0.1369863 ,\n",
       "        0.03333333],\n",
       "       [0.23529412, 0.38383838, 0.50819672, ..., 0.50670641, 0.13142123,\n",
       "        0.06666667],\n",
       "       ...,\n",
       "       [0.05882353, 0.47979798, 0.49180328, ..., 0.3561848 , 0.07534247,\n",
       "        0.01666667],\n",
       "       [0.05882353, 0.53535354, 0.62295082, ..., 0.55886736, 0.04837329,\n",
       "        0.08333333],\n",
       "       [0.58823529, 0.61616162, 0.63934426, ..., 0.41132638, 0.18321918,\n",
       "        0.4       ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_x = MinMaxScaler()\n",
    "sc_x.fit(X_train)\n",
    "sc_x.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_log_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim = 8, kernel_initializer='normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    return modelz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_nn():\n",
    "    model = build_log_model()\n",
    "    fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 429 samples, validate on 108 samples\n",
      "Epoch 1/150\n",
      "429/429 [==============================] - 0s 282us/step - loss: 0.6419 - acc: 0.6503 - val_loss: 0.6609 - val_acc: 0.6389\n",
      "Epoch 2/150\n",
      "429/429 [==============================] - 0s 60us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6653 - val_acc: 0.6296\n",
      "Epoch 3/150\n",
      "429/429 [==============================] - 0s 47us/step - loss: 0.6423 - acc: 0.6480 - val_loss: 0.6624 - val_acc: 0.6389\n",
      "Epoch 4/150\n",
      "429/429 [==============================] - 0s 51us/step - loss: 0.6426 - acc: 0.6503 - val_loss: 0.6567 - val_acc: 0.6389\n",
      "Epoch 5/150\n",
      "429/429 [==============================] - 0s 48us/step - loss: 0.6421 - acc: 0.6480 - val_loss: 0.6630 - val_acc: 0.6296\n",
      "Epoch 6/150\n",
      "429/429 [==============================] - 0s 52us/step - loss: 0.6419 - acc: 0.6503 - val_loss: 0.6572 - val_acc: 0.6389\n",
      "Epoch 7/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6600 - val_acc: 0.6389\n",
      "Epoch 8/150\n",
      "429/429 [==============================] - 0s 54us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6604 - val_acc: 0.6389\n",
      "Epoch 9/150\n",
      "429/429 [==============================] - 0s 71us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6594 - val_acc: 0.6389\n",
      "Epoch 10/150\n",
      "429/429 [==============================] - 0s 62us/step - loss: 0.6420 - acc: 0.6503 - val_loss: 0.6614 - val_acc: 0.6389\n",
      "Epoch 11/150\n",
      "429/429 [==============================] - 0s 55us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6548 - val_acc: 0.6389\n",
      "Epoch 12/150\n",
      "429/429 [==============================] - 0s 50us/step - loss: 0.6421 - acc: 0.6503 - val_loss: 0.6621 - val_acc: 0.6296\n",
      "Epoch 13/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6589 - val_acc: 0.6389\n",
      "Epoch 14/150\n",
      "429/429 [==============================] - 0s 57us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6588 - val_acc: 0.6389\n",
      "Epoch 15/150\n",
      "429/429 [==============================] - 0s 73us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6589 - val_acc: 0.6389\n",
      "Epoch 16/150\n",
      "429/429 [==============================] - 0s 55us/step - loss: 0.6419 - acc: 0.6503 - val_loss: 0.6610 - val_acc: 0.6389\n",
      "Epoch 17/150\n",
      "429/429 [==============================] - 0s 51us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6590 - val_acc: 0.6389\n",
      "Epoch 18/150\n",
      "429/429 [==============================] - 0s 53us/step - loss: 0.6423 - acc: 0.6503 - val_loss: 0.6609 - val_acc: 0.6389\n",
      "Epoch 19/150\n",
      "429/429 [==============================] - 0s 52us/step - loss: 0.6419 - acc: 0.6503 - val_loss: 0.6636 - val_acc: 0.6296\n",
      "Epoch 20/150\n",
      "429/429 [==============================] - 0s 51us/step - loss: 0.6419 - acc: 0.6503 - val_loss: 0.6600 - val_acc: 0.6389\n",
      "Epoch 21/150\n",
      "429/429 [==============================] - 0s 52us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6619 - val_acc: 0.6296\n",
      "Epoch 22/150\n",
      "429/429 [==============================] - 0s 47us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6594 - val_acc: 0.6389\n",
      "Epoch 23/150\n",
      "429/429 [==============================] - 0s 42us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6583 - val_acc: 0.6389\n",
      "Epoch 24/150\n",
      "429/429 [==============================] - 0s 46us/step - loss: 0.6420 - acc: 0.6480 - val_loss: 0.6618 - val_acc: 0.6296\n",
      "Epoch 25/150\n",
      "429/429 [==============================] - 0s 51us/step - loss: 0.6424 - acc: 0.6503 - val_loss: 0.6496 - val_acc: 0.6481\n",
      "Epoch 26/150\n",
      "429/429 [==============================] - 0s 56us/step - loss: 0.6437 - acc: 0.6503 - val_loss: 0.6749 - val_acc: 0.6389\n",
      "Epoch 27/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6447 - acc: 0.6480 - val_loss: 0.6781 - val_acc: 0.6389\n",
      "Epoch 28/150\n",
      "429/429 [==============================] - 0s 47us/step - loss: 0.6431 - acc: 0.6480 - val_loss: 0.6595 - val_acc: 0.6389\n",
      "Epoch 29/150\n",
      "429/429 [==============================] - 0s 56us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6421 - val_acc: 0.6574\n",
      "Epoch 30/150\n",
      "429/429 [==============================] - 0s 51us/step - loss: 0.6481 - acc: 0.6410 - val_loss: 0.6438 - val_acc: 0.6574\n",
      "Epoch 31/150\n",
      "429/429 [==============================] - 0s 51us/step - loss: 0.6431 - acc: 0.6480 - val_loss: 0.6711 - val_acc: 0.6389\n",
      "Epoch 32/150\n",
      "429/429 [==============================] - 0s 57us/step - loss: 0.6444 - acc: 0.6480 - val_loss: 0.6634 - val_acc: 0.6296\n",
      "Epoch 33/150\n",
      "429/429 [==============================] - 0s 73us/step - loss: 0.6419 - acc: 0.6503 - val_loss: 0.6552 - val_acc: 0.6389\n",
      "Epoch 34/150\n",
      "429/429 [==============================] - 0s 67us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6633 - val_acc: 0.6296\n",
      "Epoch 35/150\n",
      "429/429 [==============================] - 0s 87us/step - loss: 0.6421 - acc: 0.6480 - val_loss: 0.6615 - val_acc: 0.6296\n",
      "Epoch 36/150\n",
      "429/429 [==============================] - 0s 69us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6585 - val_acc: 0.6389\n",
      "Epoch 37/150\n",
      "429/429 [==============================] - 0s 61us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6607 - val_acc: 0.6296\n",
      "Epoch 38/150\n",
      "429/429 [==============================] - 0s 60us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6662 - val_acc: 0.6389\n",
      "Epoch 39/150\n",
      "429/429 [==============================] - 0s 62us/step - loss: 0.6420 - acc: 0.6503 - val_loss: 0.6610 - val_acc: 0.6296\n",
      "Epoch 40/150\n",
      "429/429 [==============================] - 0s 55us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6605 - val_acc: 0.6389\n",
      "Epoch 41/150\n",
      "429/429 [==============================] - 0s 54us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6650 - val_acc: 0.6296\n",
      "Epoch 42/150\n",
      "429/429 [==============================] - 0s 59us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6627 - val_acc: 0.6296\n",
      "Epoch 43/150\n",
      "429/429 [==============================] - 0s 54us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6602 - val_acc: 0.6389\n",
      "Epoch 44/150\n",
      "429/429 [==============================] - 0s 62us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6589 - val_acc: 0.6389\n",
      "Epoch 45/150\n",
      "429/429 [==============================] - 0s 65us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6598 - val_acc: 0.6389\n",
      "Epoch 46/150\n",
      "429/429 [==============================] - 0s 44us/step - loss: 0.6423 - acc: 0.6480 - val_loss: 0.6632 - val_acc: 0.6296\n",
      "Epoch 47/150\n",
      "429/429 [==============================] - 0s 52us/step - loss: 0.6419 - acc: 0.6503 - val_loss: 0.6572 - val_acc: 0.6389\n",
      "Epoch 48/150\n",
      "429/429 [==============================] - 0s 56us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6619 - val_acc: 0.6296\n",
      "Epoch 49/150\n",
      "429/429 [==============================] - 0s 45us/step - loss: 0.6420 - acc: 0.6503 - val_loss: 0.6638 - val_acc: 0.6296\n",
      "Epoch 50/150\n",
      "429/429 [==============================] - 0s 62us/step - loss: 0.6430 - acc: 0.6503 - val_loss: 0.6569 - val_acc: 0.6389\n",
      "Epoch 51/150\n",
      "429/429 [==============================] - 0s 55us/step - loss: 0.6423 - acc: 0.6480 - val_loss: 0.6716 - val_acc: 0.6389\n",
      "Epoch 52/150\n",
      "429/429 [==============================] - 0s 52us/step - loss: 0.6425 - acc: 0.6480 - val_loss: 0.6577 - val_acc: 0.6389\n",
      "Epoch 53/150\n",
      "429/429 [==============================] - 0s 51us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6612 - val_acc: 0.6296\n",
      "Epoch 54/150\n",
      "429/429 [==============================] - 0s 59us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6629 - val_acc: 0.6296\n",
      "Epoch 55/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6419 - acc: 0.6503 - val_loss: 0.6632 - val_acc: 0.6296\n",
      "Epoch 56/150\n",
      "429/429 [==============================] - 0s 52us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6579 - val_acc: 0.6389\n",
      "Epoch 57/150\n",
      "429/429 [==============================] - 0s 60us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6613 - val_acc: 0.6296\n",
      "Epoch 58/150\n",
      "429/429 [==============================] - 0s 54us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6622 - val_acc: 0.6296\n",
      "Epoch 59/150\n",
      "429/429 [==============================] - 0s 53us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6580 - val_acc: 0.6389\n",
      "Epoch 60/150\n",
      "429/429 [==============================] - 0s 48us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6603 - val_acc: 0.6389\n",
      "Epoch 61/150\n",
      "429/429 [==============================] - 0s 54us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6608 - val_acc: 0.6389\n",
      "Epoch 62/150\n",
      "429/429 [==============================] - 0s 48us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6577 - val_acc: 0.6389\n",
      "Epoch 63/150\n",
      "429/429 [==============================] - 0s 55us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6605 - val_acc: 0.6389\n",
      "Epoch 64/150\n",
      "429/429 [==============================] - 0s 57us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6594 - val_acc: 0.6389\n",
      "Epoch 65/150\n",
      "429/429 [==============================] - 0s 56us/step - loss: 0.6421 - acc: 0.6480 - val_loss: 0.6629 - val_acc: 0.6296\n",
      "Epoch 66/150\n",
      "429/429 [==============================] - 0s 51us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6589 - val_acc: 0.6389\n",
      "Epoch 67/150\n",
      "429/429 [==============================] - 0s 52us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6591 - val_acc: 0.6389\n",
      "Epoch 68/150\n",
      "429/429 [==============================] - 0s 52us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6622 - val_acc: 0.6296\n",
      "Epoch 69/150\n",
      "429/429 [==============================] - 0s 58us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6600 - val_acc: 0.6389\n",
      "Epoch 70/150\n",
      "429/429 [==============================] - 0s 56us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6587 - val_acc: 0.6389\n",
      "Epoch 71/150\n",
      "429/429 [==============================] - 0s 53us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6597 - val_acc: 0.6389\n",
      "Epoch 72/150\n",
      "429/429 [==============================] - 0s 59us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6605 - val_acc: 0.6389\n",
      "Epoch 73/150\n",
      "429/429 [==============================] - 0s 57us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6590 - val_acc: 0.6389\n",
      "Epoch 74/150\n",
      "429/429 [==============================] - 0s 72us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6604 - val_acc: 0.6389\n",
      "Epoch 75/150\n",
      "429/429 [==============================] - 0s 53us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6604 - val_acc: 0.6389\n",
      "Epoch 76/150\n",
      "429/429 [==============================] - 0s 52us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6593 - val_acc: 0.6389\n",
      "Epoch 77/150\n",
      "429/429 [==============================] - 0s 59us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6588 - val_acc: 0.6389\n",
      "Epoch 78/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6602 - val_acc: 0.6389\n",
      "Epoch 79/150\n",
      "429/429 [==============================] - 0s 51us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6593 - val_acc: 0.6389\n",
      "Epoch 80/150\n",
      "429/429 [==============================] - 0s 53us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6598 - val_acc: 0.6389\n",
      "Epoch 81/150\n",
      "429/429 [==============================] - 0s 59us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6586 - val_acc: 0.6389\n",
      "Epoch 82/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6624 - val_acc: 0.6296\n",
      "Epoch 83/150\n",
      "429/429 [==============================] - 0s 57us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6587 - val_acc: 0.6389\n",
      "Epoch 84/150\n",
      "429/429 [==============================] - 0s 53us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6589 - val_acc: 0.6389\n",
      "Epoch 85/150\n",
      "429/429 [==============================] - 0s 54us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6601 - val_acc: 0.6389\n",
      "Epoch 86/150\n",
      "429/429 [==============================] - 0s 69us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6576 - val_acc: 0.6389\n",
      "Epoch 87/150\n",
      "429/429 [==============================] - 0s 51us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6594 - val_acc: 0.6389\n",
      "Epoch 88/150\n",
      "429/429 [==============================] - 0s 44us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6592 - val_acc: 0.6389\n",
      "Epoch 89/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6607 - val_acc: 0.6389\n",
      "Epoch 90/150\n",
      "429/429 [==============================] - 0s 79us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6597 - val_acc: 0.6389\n",
      "Epoch 91/150\n",
      "429/429 [==============================] - 0s 57us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6580 - val_acc: 0.6389\n",
      "Epoch 92/150\n",
      "429/429 [==============================] - 0s 47us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6590 - val_acc: 0.6389\n",
      "Epoch 93/150\n",
      "429/429 [==============================] - 0s 54us/step - loss: 0.6423 - acc: 0.6480 - val_loss: 0.6608 - val_acc: 0.6389\n",
      "Epoch 94/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6420 - acc: 0.6503 - val_loss: 0.6528 - val_acc: 0.6389\n",
      "Epoch 95/150\n",
      "429/429 [==============================] - 0s 56us/step - loss: 0.6423 - acc: 0.6480 - val_loss: 0.6658 - val_acc: 0.6389\n",
      "Epoch 96/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6620 - val_acc: 0.6296\n",
      "Epoch 97/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6577 - val_acc: 0.6389\n",
      "Epoch 98/150\n",
      "429/429 [==============================] - 0s 52us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6628 - val_acc: 0.6296\n",
      "Epoch 99/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6419 - acc: 0.6480 - val_loss: 0.6621 - val_acc: 0.6296\n",
      "Epoch 100/150\n",
      "429/429 [==============================] - 0s 43us/step - loss: 0.6424 - acc: 0.6503 - val_loss: 0.6562 - val_acc: 0.6389\n",
      "Epoch 101/150\n",
      "429/429 [==============================] - 0s 47us/step - loss: 0.6419 - acc: 0.6503 - val_loss: 0.6665 - val_acc: 0.6389\n",
      "Epoch 102/150\n",
      "429/429 [==============================] - 0s 41us/step - loss: 0.6421 - acc: 0.6480 - val_loss: 0.6587 - val_acc: 0.6389\n",
      "Epoch 103/150\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6873 - acc: 0.600 - 0s 42us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6541 - val_acc: 0.6389\n",
      "Epoch 104/150\n",
      "429/429 [==============================] - 0s 47us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6602 - val_acc: 0.6389\n",
      "Epoch 105/150\n",
      "429/429 [==============================] - 0s 52us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6563 - val_acc: 0.6389\n",
      "Epoch 106/150\n",
      "429/429 [==============================] - 0s 51us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6577 - val_acc: 0.6389\n",
      "Epoch 107/150\n",
      "429/429 [==============================] - 0s 53us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6608 - val_acc: 0.6389\n",
      "Epoch 108/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6600 - val_acc: 0.6389\n",
      "Epoch 109/150\n",
      "429/429 [==============================] - 0s 63us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6553 - val_acc: 0.6389\n",
      "Epoch 110/150\n",
      "429/429 [==============================] - 0s 50us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6633 - val_acc: 0.6296\n",
      "Epoch 111/150\n",
      "429/429 [==============================] - 0s 47us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6581 - val_acc: 0.6389\n",
      "Epoch 112/150\n",
      "429/429 [==============================] - 0s 51us/step - loss: 0.6419 - acc: 0.6503 - val_loss: 0.6600 - val_acc: 0.6389\n",
      "Epoch 113/150\n",
      "429/429 [==============================] - 0s 48us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6631 - val_acc: 0.6296\n",
      "Epoch 114/150\n",
      "429/429 [==============================] - 0s 51us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6594 - val_acc: 0.6389\n",
      "Epoch 115/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6579 - val_acc: 0.6389\n",
      "Epoch 116/150\n",
      "429/429 [==============================] - 0s 56us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6590 - val_acc: 0.6389\n",
      "Epoch 117/150\n",
      "429/429 [==============================] - 0s 46us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6604 - val_acc: 0.6389\n",
      "Epoch 118/150\n",
      "429/429 [==============================] - 0s 48us/step - loss: 0.6418 - acc: 0.6503 - val_loss: 0.6599 - val_acc: 0.6389\n",
      "Epoch 119/150\n",
      "429/429 [==============================] - 0s 52us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6611 - val_acc: 0.6296\n",
      "Epoch 120/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6594 - val_acc: 0.6389\n",
      "Epoch 121/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6597 - val_acc: 0.6389\n",
      "Epoch 122/150\n",
      "429/429 [==============================] - 0s 45us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6601 - val_acc: 0.6389\n",
      "Epoch 123/150\n",
      "429/429 [==============================] - 0s 48us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6592 - val_acc: 0.6389\n",
      "Epoch 124/150\n",
      "429/429 [==============================] - 0s 48us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6612 - val_acc: 0.6296\n",
      "Epoch 125/150\n",
      "429/429 [==============================] - 0s 46us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6590 - val_acc: 0.6389\n",
      "Epoch 126/150\n",
      "429/429 [==============================] - 0s 58us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6587 - val_acc: 0.6389\n",
      "Epoch 127/150\n",
      "429/429 [==============================] - 0s 53us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6597 - val_acc: 0.6389\n",
      "Epoch 128/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6593 - val_acc: 0.6389\n",
      "Epoch 129/150\n",
      "429/429 [==============================] - 0s 41us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6580 - val_acc: 0.6389\n",
      "Epoch 130/150\n",
      "429/429 [==============================] - 0s 53us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6597 - val_acc: 0.6389\n",
      "Epoch 131/150\n",
      "429/429 [==============================] - 0s 43us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6594 - val_acc: 0.6389\n",
      "Epoch 132/150\n",
      "429/429 [==============================] - 0s 45us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6579 - val_acc: 0.6389\n",
      "Epoch 133/150\n",
      "429/429 [==============================] - 0s 44us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6576 - val_acc: 0.6389\n",
      "Epoch 134/150\n",
      "429/429 [==============================] - 0s 44us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6605 - val_acc: 0.6389\n",
      "Epoch 135/150\n",
      "429/429 [==============================] - 0s 48us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6592 - val_acc: 0.6389\n",
      "Epoch 136/150\n",
      "429/429 [==============================] - 0s 42us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6574 - val_acc: 0.6389\n",
      "Epoch 137/150\n",
      "429/429 [==============================] - 0s 46us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6602 - val_acc: 0.6389\n",
      "Epoch 138/150\n",
      "429/429 [==============================] - 0s 46us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6591 - val_acc: 0.6389\n",
      "Epoch 139/150\n",
      "429/429 [==============================] - 0s 40us/step - loss: 0.6415 - acc: 0.6503 - val_loss: 0.6577 - val_acc: 0.6389\n",
      "Epoch 140/150\n",
      "429/429 [==============================] - 0s 51us/step - loss: 0.6417 - acc: 0.6503 - val_loss: 0.6587 - val_acc: 0.6389\n",
      "Epoch 141/150\n",
      "429/429 [==============================] - 0s 44us/step - loss: 0.6415 - acc: 0.6503 - val_loss: 0.6587 - val_acc: 0.6389\n",
      "Epoch 142/150\n",
      "429/429 [==============================] - 0s 50us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6603 - val_acc: 0.6389\n",
      "Epoch 143/150\n",
      "429/429 [==============================] - 0s 44us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6573 - val_acc: 0.6389\n",
      "Epoch 144/150\n",
      "429/429 [==============================] - 0s 52us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6603 - val_acc: 0.6389\n",
      "Epoch 145/150\n",
      "429/429 [==============================] - 0s 42us/step - loss: 0.6415 - acc: 0.6503 - val_loss: 0.6609 - val_acc: 0.6389\n",
      "Epoch 146/150\n",
      "429/429 [==============================] - 0s 48us/step - loss: 0.6416 - acc: 0.6503 - val_loss: 0.6597 - val_acc: 0.6389\n",
      "Epoch 147/150\n",
      "429/429 [==============================] - 0s 49us/step - loss: 0.6419 - acc: 0.6503 - val_loss: 0.6581 - val_acc: 0.6389\n",
      "Epoch 148/150\n",
      "429/429 [==============================] - 0s 41us/step - loss: 0.6429 - acc: 0.6480 - val_loss: 0.6671 - val_acc: 0.6389\n",
      "Epoch 149/150\n",
      "429/429 [==============================] - 0s 57us/step - loss: 0.6423 - acc: 0.6503 - val_loss: 0.6567 - val_acc: 0.6389\n",
      "Epoch 150/150\n",
      "429/429 [==============================] - 0s 46us/step - loss: 0.6428 - acc: 0.6480 - val_loss: 0.6658 - val_acc: 0.6389\n"
     ]
    }
   ],
   "source": [
    "log_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
